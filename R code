#Understanding the behavior of IT companies through Machine Learning
#Sebastien Verpile

#################################### Basics ##################################### 
library(readr) 
library(dplyr)
library(magrittr) 
library(ggplot2)
library(VIM)
library(mice)
library(lattice)
library(psych)
library(e1071)
library(caret)
library(cluster)

xdf <- read_csv("C:/Users/Sebastien Verpile/Desktop/CUS615/Project/xdf.csv", 
                col_types = cols(bvps = col_number(), 
                                 cfps = col_number(), dps = col_number(), 
                                 eps = col_number(), epsf = col_number(), 
                                 mv = col_number(), price = col_number(), 
                                 rptDate = col_character()))

data<-tbl_df(xdf)# convert data frame into table fomat for dplyr

#The code above shows the packages that are going to be used for data analysis and also how the data was upload into R:

#readr->uploading data into R
#dplyr->data manipulation
#magrittr->chaining commands
#ggplot2-> visualizations
#VIM-> missing values
#mice-> missing values
#laticce-> density plot
#psych-> descriptive stats
#e1071-> Naive Bayes classifier 
#caret-> Classifiers

############################################# Attirbute filtering ############################### 

df<-data %>%
  select(cusip:dps,contains("sp500"), contains("rptDate")) %>%
  filter(sp500==1, sector=="Information Technology")

#From the "df" data frame created, I selected all the attributes for the SP500 companies in the Information Technology sector. 

################################### Raw data exploration ##################################### 

dimension.raw<-dim(df)
variables.raw<-names(df)
companies.raw<-unique(df$name)
industries.raw<-unique(df$industry)

#In the code chunk above, I created different object that contain the dimensions, the variable names, the company names, and the industry names in the data. 

########################################### Dimensions ###################################
dimension.raw 

#1139 tuples and 15 attributes. 

#####################################  Variable names ####################### 

variables.raw %>% data.frame

#Varible descriptions:
    #cusip: Company ID.  
    #name: Company name. 
    #date: end of the year.
    #tkr: Company symbol. 
    #sector: Business sector. 
    #industry: Company industry. 
    #mv: Market Value.
    #price: Company share price.
    #eps: Company earnigns per share. 
    #epsf: Earning per share forward.
    #cfps: Cash flow per share. 
    #bvps: Book value per share. 
    #dps: Dividends per share. 
    #sp500: Whether or not company is in SP500.
    #rptDate: Report date. 
    
###################################### Company names ############################

companies.raw %>% data.frame

#There are 79 companies in total. 

#################################### industry names #########################
#There are 7 different industries. 

# Industry (class) merging 
n.industries.raw<-df %>% count(industry)
n.industries.raw

df$industry<-gsub("Internet Software & Services", "IT Services", df$industry) #Internet Software & Services->IT Services
df$industry<-gsub("Semiconductors & Semiconductor Equipment", "Communications Equipment",df$industry)

#Semiconductors & Semiconductor Equipment->Communications Equipment
df$industry<-gsub("Electronic Equipment Instruments & Components", "Communications Equipment",df$industry)

#Electronic Equipment Instruments & Components->Communications Equipment
df$industry<-gsub("Technology Hardware Storage & Peripherals", "Software", df$industry) #Technology Hardware Storage & Peripherals->Software


#Earlier we saw that there were seven different industries. In order to get more accurate results in the classification and clustering models, I decided to merge similar classes together in order to have fewer classes. The goal here is to end up with only 3 classes. After looking up what the different companies do, I decided to implement the following merges: "Internet software & Services" will be merged to "IT Services"; "Semiconductors & Semiconductor Equipement" will be merged with "Communications equipement"; "Electronic Equipment Instruments & Components" will be merged with "Communications equipement"; and finally Technology Hardware Storage & Peripherals will merge with Software. 

merged<-df%>%count(industry)

#Above, we can see the resulting classes after going through the merging process.  

# Dealing with missing values 
#In any data analysis, missing values can become a problem especially when they are a lot of them. 

#When we look at the missing value matrix below, we can see that there are only 945 tuples that have no missing values. This means that if we were to remove the missing values completely, we would lose 194 observations. Loosing this much data can become a problem so we are better off imputing new data for the missing values. 

df %>% select(mv, eps, cfps, dps, price, epsf, bvps) %>% md.pattern

#If we want to understand the missing value pattern matrix in a more visual way, the graphs below displays the missing values with their respectives percentage counts. 

numeric.vars<- df %>% select(cfps,eps,epsf,price,mv,dps,bvps)#Numeric variables

aggr_plot <- aggr(numeric.vars, col=c('forestgreen','black'), numbers=FALSE,
sortVars=TRUE, labels=names(numeric.vars), cex.axis=2.25, cex.lab=1.75, gap=3,
ylab=c("Histogram of missing data","Pattern"))

p<-df %>% ggplot(aes(industry, price, fill=industry))
p+ stat_boxplot()+
  labs(title= "Price/industry",y="Price")+
  theme_classic()+
  theme(legend.title = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        legend.justification=c(1,1),legend.position=c(1,1))


eps<-df %>% ggplot(aes(industry, eps, fill=industry))
eps+ stat_boxplot()+
  theme_classic()+ 
  labs(title= "Earnings per share/industry",y="Eps")+
     theme(legend.title = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        legend.justification=c(1,0),legend.position=c(1,0))


epsf<-df %>% ggplot(aes(industry, epsf, fill=industry))
epsf+ stat_boxplot()+
  labs(title= "Earnigns per share forward/industry",y="Epsf")+
  theme_classic()+
       theme(legend.title = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        legend.justification=c(1,1),legend.position=c(1,1))

epsf<-df %>% ggplot(aes(industry, mv, fill=industry))
epsf+ stat_boxplot()+
  labs(title= "Market value/industry",y="Mv")+
  theme_classic()+
       theme(legend.title = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        legend.justification=c(0,1),legend.position=c(0,1))

epsf<-df %>% ggplot(aes(industry, cfps, fill=industry))
epsf+ stat_boxplot()+
  labs(title= "Cashflow per share/industry",y="Cfps")+
  theme_classic()+
       theme(legend.title = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        legend.justification=c(1,1),legend.position=c(1,1))
        

epsf<-df %>% ggplot(aes(industry, bvps, fill=industry))
epsf+ stat_boxplot()+
  labs(title= "Book value per share/industry",y="Bvps")+
  theme_classic()+
       theme(legend.title = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        legend.justification=c(1,1),legend.position=c(1,1))


epsf<-df %>% ggplot(aes(industry,dps, fill=industry))
epsf+ stat_boxplot()+
  labs(title= "Dividends per share/industry",y="Dps")+
  theme_classic()+
       theme(legend.title = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        legend.justification=c(1,1),legend.position=c(1,1))

#When we look at the boxplots for the numerical variables in the data, we can see that there are a lot of outliers. So, when we are replacing the missing values, we might not want to use the attribute average because this average will be biased because of the outliers. 


tempData <- mice(numeric.vars,m=5,maxit=5,meth='sample',seed=500)
summary(tempData)



#In the code above, the method used to generate the random value is "sample". Meaning that a random samples from the observed values were chosen at random to fill the missing values.
#Now, let us look at how well the imputed data:

densityplot(tempData)

stripplot(tempData, pch = 20, cex = 1.2,xlab="")#main="Strip plot of imputed data", cex.main=6)

#The plots above will help us decide which random sample fits the original data better. Even though it looks like all the random samples generated fite the original data really well, I decided to use the 3rd sample generated. Now let's complete the datset: 

completedData<-complete(tempData,3)

#Let's check for missing values:

pMiss <- function(x){sum(is.na(x))} #Function that calculates number of missing values 
apply(completedData,2,pMiss)

#When we look at the missing values matrix for the completed data, we can see that all the missing values have been imputed. 

#Before we start fitting machine learning models, we are going to create the final data frame that will be used throughout the project.

finaldf<-df %>% select(industry) %>% bind_cols(completedData)# Final df


par(mfrow=c(3,3))
hist(finaldf$price, main="Price",cex.main=1.5, ylab=NULL, xlab=NULL,col="green", prob=T)
lines(density(finaldf$price), col="red")
hist(finaldf$mv, main="Market value", cex.main=1.5,ylab=NULL, xlab=NULL, col="green")
hist(finaldf$eps,main="Earnings/share",cex.main=1.5,ylab=NULL, xlab=NULL, col="green")
lines(density(finaldf$eps), col="red")
hist(finaldf$epsf,main="Earnings/share forward",cex.main=1.5,ylab=NULL, xlab=NULL, col="green")
hist(finaldf$cfps,main="Cash flow/share",cex.main=1.5,ylab=NULL, xlab=NULL, col="green")
hist(finaldf$bvps,main="Book value/share",cex.main=1.5,ylab=NULL, xlab=NULL, col="green")
hist(finaldf$dps,main="Dividends/share",cex.main=1.5,ylab=NULL, xlab=NULL, col="green")


par(mfrow=c(3,3))
a <- density(finaldf$cfps)
plot(a, main="Cashflow per share", xlab="", ylab="", cex.main=1.5)
polygon(a, col="green", border="green")

b <- density(finaldf$eps)
plot(b, main="Earnings/share" ,xlab="", ylab="", cex.main=1.5)
polygon(b, col="green", border="green")

c <- density(finaldf$epsf)
plot(c, main="Earnings/share forward", xlab="", ylab="", cex.main=1.5)
polygon(c, col="green", border="green")

d <- density(finaldf$price)
plot(d, main="Price", xlab="", ylab="", cex.main=1.5)
polygon(d, col="green", border="green")

e <- density(finaldf$mv)
plot(e, main="Market value", xlab="", ylab="", cex.main=1.5)
polygon(e, col="green", border="green")

f <- density(finaldf$dps)
plot(f, main="Dividends/share", xlab="", ylab="", cex.main=1.5)
polygon(f, col="green", border="green")

g <- density(finaldf$bvps)
plot(g, main="Book value/share", xlab="", ylab="", cex.main=1.5)
polygon(g, col="green", border="green")



par(mfrow=c(3,3))
boxplot(finaldf$mv, main="Market value", cex.main=1.5,ylab=NULL, xlab=NULL, col="black")
boxplot(finaldf$price, main="Price",cex.main=1.5, ylab=NULL, xlab=NULL, col="black")
boxplot(finaldf$eps,main="Earnings/share",cex.main=1.5,ylab=NULL, xlab=NULL, col="black")
boxplot(finaldf$epsf,main="Earnings/share forward",cex.main=1.5,ylab=NULL, xlab=NULL, col="black")
boxplot(finaldf$cfps,main="Cash flow/share",cex.main=1.5,ylab=NULL, xlab=NULL, col="black")
boxplot(finaldf$bvps,main="Book value/share",cex.main=1.5,ylab=NULL, xlab=NULL, col="black")
boxplot(finaldf$dps,main="Dividends/share",cex.main=1.5,ylab=NULL, xlab=NULL, col="black")



#Now that we have our final dataset, we can start looking at some descriptive statistics for the data: 

fdesstat<-function(inputDataFrame){
  meanVec<-apply(inputDataFrame,2,mean,na.rm=T)
  medVec<-apply(inputDataFrame,2,median,na.rm=T)
  sdVec<-apply(inputDataFrame,2,SD,na.rm=T)
  skewVec<-apply(inputDataFrame,2,skew,na.rm=T)
  kurtVec<-apply(inputDataFrame,2,kurtosi,na.rm=T)
  nVec<-apply(inputDataFrame,2,length)
  resultsDF<-data.frame(cbind(meanVec,medVec,sdVec,skewVec,kurtVec,nVec))
  names(resultsDF)<-c("mean","med","stdev","skew","kurt","n")
  return(resultsDF)}

summary_stats<-finaldf %>% select(cfps:bvps) %>% fdesstat() %>% round(2)
summary_stats 

#In order to apply machine learning algorithms to the data, we must first divide the dataset into training and test sets repectively. Even though there are several algorithms that can be applied to split the data, I am going to use a 60-40 method to split the data. With 60% for training and 40% for testing. 
#The code below will split the data into training and test sets using a random sampling method, without replacement. 
#The code will also display the dimensions of training and test sets. 

############################# Naive Bayes Classifier ########################### 
####60-40 sampling with replacement

i<-1:200
results_f<-c(1:200)
for (i in results_f){
train<- finaldf %>% dplyr::sample_frac(0.6, replace=F) 
test<- finaldf %>% dplyr::sample_frac(0.4, replace=F)
train$industry<-as.factor(train$industry)
test$industry<-as.factor(test$industry)
model<-e1071::naiveBayes(industry~.,data=train[,1:8], laplace=2)
predictions<-predict(model, test[,2:8])
results_f[i]<-sum(predictions==test$industry)/length(predictions)*100
}
table(predictions,test$industry) 
confusionMatrix(predictions,test$industry)
accuracy<-max(round(results_f,1))
accuracy



########################################### KNN classification ##############################3
## Repeated cross validation sampling 


# Splitting data 
intrain <- createDataPartition(y = finaldf$industry, p= 0.6, list = FALSE)
training <- finaldf[intrain,] 
testing <- finaldf[-intrain,]

# Model implementation
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
knn_fit <- train(industry~., data = training, method = "knn",
 trControl=trctrl,
 preProcess = c("center", "scale"),
 tuneLength = 10)
knn_fit

#Plotting accuracy vs. K
K<-knn_fit$results$k
Accuracy<-knn_fit$results$Accuracy
plot(K,Accuracy, main="K vs. Accuracy", col="blue", pch=20)
lines(K,Accuracy, type = "l", col="blue")

#Predictions
test_pred <- predict(knn_fit, newdata = testing)
confusionMatrix(test_pred, testing$industry)


############################################## K-means Clustering #######################################3

set.seed(1)
isGoodCol<-function(col){
  sum(is.na(col))== 0 && is.numeric(col)
}
goodCols<-sapply(finaldf,isGoodCol)

clusters<-kmeans(finaldf[,goodCols], centers=3)
labels<-clusters$cluster
finaldf2d <- prcomp(finaldf[,goodCols], center=TRUE)
twoColumns <- finaldf2d$x[,1:2]
clusplot(twoColumns, labels, 
         main= "Clusters K=3", 
         cex.main=2 ,
         #xlab="", ylab="", sub="",
         lines=0,
         col.p=labels)




##########################Summary statistics####### 
library(psych)
fdesstat<-function(inputDataFrame){
  meanVec<-apply(inputDataFrame,2,mean,na.rm=T)
  medVec<-apply(inputDataFrame,2,median,na.rm=T)
  sdVec<-apply(inputDataFrame,2,SD,na.rm=T)
  skewVec<-apply(inputDataFrame,2,skew,na.rm=T)
  kurtVec<-apply(inputDataFrame,2,kurtosi,na.rm=T)
  nVec<-apply(inputDataFrame,2,length)
  resultsDF<-data.frame(cbind(meanVec,medVec,sdVec,skewVec,kurtVec,nVec))
  names(resultsDF)<-c("mean","med","stdev","skew","kurt","n")
  return(resultsDF)}

summary_stats<-df %>% select(mv:dps) %>% fdesstat() %>% round(2)



#Functions
panel.cor <- function(x, y, digits=2, cex.cor)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y))
  txt <- format(c(r, 0.123456789), digits=digits)[1]
  test <- cor.test(x,y)
  Signif <- ifelse(round(test$p.value,3)<0.001,"p<0.001",paste("p=",round(test$p.value,3)))  
  text(0.5, 0.25, paste("r=",txt))
  text(.5, .75, Signif)
}

panel.smooth<-function (x, y, col = "blue", bg = NA, pch = 18, 
                        cex = 0.8, col.smooth = "red", span = 2/3, iter = 3, ...) 
{
  points(x, y, pch = pch, col = col, bg = bg, cex = cex)
  ok <- is.finite(x) & is.finite(y)
  if (any(ok)) 
    lines(stats::lowess(x[ok], y[ok], f = span, iter = iter), 
          col = col.smooth, ...)
}

panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col="green", ...)
}

#Final plot
pairs(d[,6:12],
      lower.panel=panel.smooth, 
      upper.panel=panel.cor,diag.panel=panel.hist)



#Correlation plots#
library(GGally)
library(corrplot)
library(RColorBrewer)


cor<-round(cor(fdf[,-1]),2)

# mat : is a matrix of data
# ... : further arguments to pass to the native R cor.test function
cor.mtest <- function(mat, ...) {
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat<- matrix(NA, n, n)
  diag(p.mat) <- 0
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      tmp <- cor.test(mat[, i], mat[, j], ...)
      p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
    }
  }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
# matrix of the p-value of the correlation
p.mat <- cor.mtest(d[,6:12])

#plot1
corrplot(cor, method="color", col=brewer.pal(n=6, name="PuOr"),  
         type="lower", order="hclust", 
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=45, #Text label color and rotation
         # Combine with significance
         p.mat = p.mat, sig.level = 0.01, insig = "blank", 
         # hide correlation coefficient on the principal diagonal
         diag=FALSE 
)

#plot
ggpairs(d[,6:12], lower=list(continuous="smooth_loess"))


library(caret)

# summarize dataset
summary(fdf)
# calculate the pre-process parameters from the dataset
preprocessParams <- preProcess(fdf[,2:8], method=c("pca"))
# summarize transform parameters
print(preprocessParams)
# transform the dataset using the parameters
transformed <- predict(preprocessParams, fdf[,2:8])
# summarize the transformed dataset
summary(transformed)

# Create new data frame
var1<-as.numeric(transformed$PC1)
var2<-as.numeric(transformed$PC2)
var3<-as.numeric(transformed$PC3)
var4<-as.numeric(transformed$PC4)
var5<-as.numeric(transformed$PC5)
var6<-as.numeric(transformed$PC6)
class<-fdf$industry

new<-data.frame(cbind(fdf$industry, var1, var2,var3,
                      var4,var5,var6))
new$var1<-as.numeric(new$var1)
new$var2<-as.numeric(new$var2)
new$var3<-as.numeric(new$var3)
new$var4<-as.numeric(new$var4)
new$var5<-as.numeric(new$var5)
new$var6<-as.numeric(new$var6)


# Naive bayes classifier with PCA
intrain <- createDataPartition(y = new$V1, p= 0.6, list = FALSE)
training <- new[intrain,] 
testing <- new[-intrain,]

trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
nb_fit <- train(V1~., data = training, method = "nb",
                trControl=trctrl,
                preProcess = c("center","scale"))
nb_fit

nb_pred <- predict(nb_fit, newdata = testing)
confusionMatrix(nb_pred, testing$V1)

#Random forest PCA
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
rf<- train(V1~., data = training, method = "rf",
           trControl=trctrl,
           preProcess = c("center","scale"))
rf

rf_pred <- predict(rf, newdata = testing)
confusionMatrix(rf_pred, testing$V1)


#Knn PCA
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
knn_fit <- train(V1~., data = training, method = "knn",
                 trControl=trctrl,
                 preProcess = c("center","scale"),
                 tuneLength = 10)
knn_fit

#Plotting accuracy vs. K
trellis.par.set(caretTheme())
plot(knn_fit,main="K vs. Accuracy", col="red", pch=20,
     xlab="Neighbors", ylab="Accuracy") 

#ggplot(knn_fit)

#Predictions
test_pred <- predict(knn_fit, newdata = testing)
confusionMatrix(test_pred, testing$V1)

